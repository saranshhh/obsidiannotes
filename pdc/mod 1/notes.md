Parallel computing ->   process of executing several processors an application or computation simultaneously; architecture where long big tasks are broken down into smaller independent tasks which can be processed together.
Real-world data needs more dynamic simulation and modeling, and for achieving the same, parallel computing is the key.
Complex, large datasets, and their management can be organized and using parallel computing’s approach.
-> Traditonally, computers were built for serial computing; ie given command; broken into smaller instructions & processed one by one.
![[Pasted image 20250212192011.png]]
Diagram for serial computing

**Parallel Computing**
using mulitiple resources at a given time to optimise problems in computing,
~ run on different CPUs; i.e. different threads & processes on the microprocessor
Problem broken down into different smalller instructions to be processed togther.
![[Pasted image 20250212192226.png]]
Advantages:
1. faster application processing
2. task resolution is faster 
3. increasing available computation power of the system
4. saves time & money in the long run
5. provide concurrency
6. solve larger problems
7. reduces complexity
Use case: used by supercomputers; where huge requirements of computation
-> ex: comparison between 1 queue vs 2 queue

Parallel computing can be achieved by:
1. A single computer with multiple processors; 
2. An arbitrary number of computers connected by a network; 
3. A combination of both
Types of parallel computing:
1. Bit-level parallelism: The form of parallel computing in which every task is dependent on processor word size.
	1. In terms of performing a task on large-sized data, it reduces the number of instructions the processor must execute. There is a need to split the operation into series of instructions.


One of the primary applications of parallel computing is Databases and Data mining.
The concept of parallel computing is used by augmented reality, advanced graphics, and virtual reality.
• True Advantages:
1. In parallel computing, more resources are used to complete the task that led to decrease the time and cut possible costs.
2. solve larger problems
3. For simulating, modeling, and understanding complex, real- world phenomena, parallel computing is much appropriate while comparing with serial computing.
4. Parallel computing is suited for hardware as serial computing wastes the potential computing power.

• Dis:
1. It addresses Parallel architecture that can be difficult to achieve. 
2. In the case of clusters, better cooling technologies are needed in parallel computing. 
3. It requires the managed algorithms, which could be handled in the parallel mechanism. 
4. The multi-core architectures consume high power consumption. 
5. The parallel computing system needs low coupling and high cohesion, which is difficult to create. 
6. The code for a parallelism-based program can be done by the most technically skilled and expert programmer.
7. Although parallel computing helps you out to resolve computationally and the data-exhaustive issue with the help of using multiple processors, sometimes it affects the conjunction of the system and some of our control algorithms and does not provide good outcomes due to the parallel option.
8. Due to synchronization, thread creation, data transfers, and more, the extra cost sometimes can be quite large; even it may be exceeding the gains because of parallelization. 
9. Moreover, for improving performance, the parallel computing system needs different code tweaking for different target architectures

**HIGH PERFORMANCE COMPUTING**
Parallel Computers: Networks connect multiple stand-alone computers (nodes) to create larger parallel computer clusters
Parallel systems deal with the simultaneous use of multiple computer resources that can include a single computer with multiple processors, a number of computers connected by a network to form a parallel processing cluster, or a combination of both.
SHARED & DISTRIBUTED MEMORY