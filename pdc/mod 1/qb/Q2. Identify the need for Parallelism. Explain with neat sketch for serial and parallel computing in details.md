Parallelism in computer architecture is ==the process of dividing a task into multiple parts and executing them simultaneously using multiple processors==. This approach is also known as parallel processing or parallel computing.
 Need for parallelism:
- **Solve complex problems faster:** Many real-world problems, such as weather forecasting, protein folding, and financial modeling, involve massive amounts of data and complex calculations. Parallelism allows these tasks to be broken down into smaller, independent subtasks that can be executed simultaneously, significantly reducing the overall execution time.
- **Handle massive datasets:** With the explosion of data in fields like big data analytics, machine learning, and scientific simulations, traditional sequential processing becomes inadequate. Parallel computing enables the processing of these vast datasets by distributing the workload across multiple processors or cores.
- **Improve responsiveness:** In applications requiring real-time processing, such as online gaming, video streaming, and autonomous driving, parallelism can enhance responsiveness by handling multiple tasks concurrently.
- **Maximize hardware utilization:** Modern computers often have multiple cores or processors. Parallelism allows these resources to be used more efficiently, leading to better overall performance.
- **Simulate complex systems:** Scientific and engineering simulations often involve modeling intricate systems with numerous interacting components. Parallel computing enables the simulation of these systems by dividing the model into smaller parts that can be simulated simultaneously.
- **Achieve higher throughput:** In server applications and data processing pipelines, parallelism can increase throughput by handling multiple requests or data streams concurrently.
- **Address the limitations of sequential processing:** As clock speeds of single-core processors have reached their physical limits, increasing performance through parallelism has become the primary approach.
- **Improve fault tolerance:** In some parallel systems, redundancy can be built in, so if one processor fails, others can take over its workload, increasing overall system reliability.

In essence, the need for parallelism stems from the ever-increasing demand for faster, more efficient, and more powerful computing capabilities.
 
 Traditionally, computers were built for serial computing; ie given command; broken into smaller instructions & processed one by one. on a single CPU.
 ![[Pasted image 20250212192011.png]]
**Parallel Computing**
using multiple resources at a given time to optimize problems in computing,
~ run on different CPUs; i.e. different threads & processes on the microprocessor
Problem broken down into different smaller instructions to be processed together.
![[Pasted image 20250212192226.png]]

|Parameters|Sequential Computing|Parallel Computing|
|---|---|---|
|1.|All the instructions are executed in a sequence, one at a time.|All the instructions are executed parallelly.|
|2.|It has a single processor.|It is having multiple processors.|
|3.|It has low performance and the workload of the processor is high due to the single processor.|It has high performance and the workload of the processor is low because multiple processors are working simultaneously.|
|4.|Bit-by-bit format is used for data transfer.|Data transfers are in bytes.|
|5.|It requires more time to complete the whole process.|It requires less time to complete the whole process.|
|6.|Cost is low|Cost is high|